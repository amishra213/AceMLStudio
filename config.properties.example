# AceML Studio Configuration Properties
# =======================================
# Copy this file to 'config.properties' and fill in your actual values
# DO NOT commit the 'config.properties' file to version control

# Application Settings
SECRET_KEY=your-secret-key-here-change-in-production
DEBUG=True

# LLM Provider Selection
# Supported: openai, azure_openai, anthropic, deepseek
LLM_PROVIDER=deepseek

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=2048
OPENAI_TEMPERATURE=0.3

# Azure OpenAI Configuration
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_KEY=your-azure-openai-key-here
AZURE_OPENAI_DEPLOYMENT=your-deployment-name
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Anthropic Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# DeepSeek Configuration
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MAX_TOKENS=2048
DEEPSEEK_TEMPERATURE=0.3

# =======================================
# File Upload Configuration
# =======================================
# Maximum file size allowed for upload (in MB)
MAX_FILE_UPLOAD_SIZE_MB=256

# Chunk size for large file uploads (in MB)
CHUNK_SIZE_MB=5

# Files larger than this threshold will use chunked upload (in MB)
LARGE_FILE_THRESHOLD_MB=50

# Use database storage when RAM is insufficient
USE_DB_FOR_LARGE_FILES=True

# Memory threshold for switching to database (in MB)
DB_FALLBACK_THRESHOLD_MB=500

# =======================================
# Cloud GPU Configuration
# =======================================
# Enable cloud GPU for model training and hyperparameter tuning
# Set to True to use cloud GPUs, False to use local compute
CLOUD_GPU_ENABLED=False

# Cloud Provider Selection
# Supported: aws_sagemaker, azure_ml, gcp_vertex, custom
CLOUD_GPU_PROVIDER=aws_sagemaker

# ──────────────────────────── AWS SageMaker ────────────────────────────
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your-aws-access-key-id
AWS_SECRET_ACCESS_KEY=your-aws-secret-access-key
AWS_SAGEMAKER_ROLE_ARN=arn:aws:iam::123456789012:role/SageMakerRole
AWS_SAGEMAKER_INSTANCE_TYPE=ml.p3.2xlarge
AWS_SAGEMAKER_S3_BUCKET=your-sagemaker-bucket

# ──────────────────────────── Azure ML ────────────────────────────────
AZURE_SUBSCRIPTION_ID=your-azure-subscription-id
AZURE_RESOURCE_GROUP=your-resource-group
AZURE_WORKSPACE_NAME=your-workspace-name
AZURE_TENANT_ID=your-tenant-id
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret
AZURE_COMPUTE_TARGET=gpu-cluster
AZURE_VM_SIZE=Standard_NC6

# ──────────────────────────── GCP Vertex AI ────────────────────────────
GCP_PROJECT_ID=your-gcp-project-id
GCP_REGION=us-central1
GCP_SERVICE_ACCOUNT_KEY_PATH=path/to/service-account-key.json
GCP_MACHINE_TYPE=n1-standard-4
GCP_ACCELERATOR_TYPE=NVIDIA_TESLA_T4
GCP_ACCELERATOR_COUNT=1

# ──────────────────────────── Custom GPU Server ────────────────────────
# For private cloud or remote GPU servers (e.g., Lambda Labs, Paperspace)
CUSTOM_GPU_ENDPOINT=https://your-gpu-server.com/api
CUSTOM_GPU_API_KEY=your-custom-gpu-api-key
CUSTOM_GPU_AUTH_TOKEN=your-auth-token
CUSTOM_GPU_USERNAME=your_username
CUSTOM_GPU_PASSWORD=your_password

# ──────────────────────────── GPU Training Options ────────────────────
# Timeout for cloud GPU jobs (seconds)
GPU_JOB_TIMEOUT=3600
# Auto-detect GPU availability and fall back to local if cloud fails
GPU_FALLBACK_TO_LOCAL=True
# Max retries for cloud GPU job submission
GPU_MAX_RETRIES=3
